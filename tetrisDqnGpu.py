# -*- coding: utf-8 -*-
"""tetrisDqn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pRtBADORBUCa87z-e9NoiEYvAv6qkS-z
"""

import torch
from torch import nn
from torch import optim
from collections import namedtuple, deque
import random
import math

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)
print(torch.cuda.get_device_name(0))

game_rows = 12
game_cols = 3
middle = math.floor(game_cols/2)

Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

class ExperienceReplay(object):
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

channels = 2
neurons = 128
filter_size = 3

class DqnNet(nn.Module):
  def __init__(self):
    super(DqnNet, self).__init__()
    self.feed = nn.Sequential(
        nn.Flatten(),
        nn.Linear(channels*game_rows*game_cols, neurons),
        nn.LeakyReLU(),
        nn.Linear(neurons, neurons),
        nn.LeakyReLU(),
        nn.Linear(neurons, neurons),
        nn.LeakyReLU(),
        nn.Linear(neurons, 3)
    )

  def forward(self, x):
    return self.feed(x)

policy_net = DqnNet().to(device)
print(f"policy_net on cuda: {next(policy_net.parameters()).is_cuda}")
target_net = DqnNet().to(device)
print(f"target_net on cuda: {next(target_net.parameters()).is_cuda}")
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()
loss_function = nn.SmoothL1Loss()  # Huber Loss
optimizer = optim.RMSprop(policy_net.parameters())
memory = ExperienceReplay(100000)

def create_piece(state):
  state[0, 0, 0, middle] = 1
  #state[0, 0, 0, middle-1] = 1

def move_piece(state, action):
  state[0, 0] = torch.roll(state[0, 0], 1, 0)
  if action == 1:
    if not(0 in torch.index_select(state[0, 0].nonzero(), 1, torch.tensor([1]).to(device))):
      state[0, 0] = torch.roll(state[0, 0], -1, 1)
      if 2 in torch.sum(state, dim=1):
        state[0, 0] = torch.roll(state[0, 0], 1, 1)
  if action == 2:
    if not(game_cols-1 in torch.index_select(state[0, 0].nonzero(), 1, torch.tensor([1]).to(device))):
      state[0, 0] = torch.roll(state[0, 0], 1, 1)
      if 2 in torch.sum(state, dim=1):
        state[0, 0] = torch.roll(state[0, 0], -1, 1)

def piece_landed(state):
  landed = False
  state[0, 0] = torch.roll(state[0, 0], 1, 0)
  if 2 in torch.sum(state, dim=1):
    landed = True
  state[0, 0] = torch.roll(state[0, 0], -1, 0)
  if game_rows-1 in torch.index_select(state[0, 0].nonzero(), 1, torch.tensor([0]).to(device)):
    landed = True
  return landed

def transfer_piece(state):
  state[0, 1] = torch.sum(state, dim=1)
  state[0, 0] = 0

def game_over():
  return True if torch.sum(state[0, 1, 1, :]).item() > 0 else False

def restart_game():
  state = torch.zeros(1, 2, game_rows, game_cols)
  state = state.to(device)
  create_piece(state)
  return state

def score(state):
  scored_rows = 0
  for row in range(game_rows):
    if torch.sum(state[0, 1, row, :]) == game_cols:
      scored_rows += 1
      tmp = torch.clone(state)
      state[0, 1, 1:row+1, :] = tmp[0, 1, 0:row, :]
  return scored_rows

epsilon = 1
epsilon_end = 0.05
decay = 0.999
discount = 0.99
random.seed()

def select_action(state):
  global epsilon
  rand = random.random()
  if rand <= epsilon:
      action = random.randrange(0, 3)
      if epsilon > epsilon_end:
        epsilon *= decay
  else:
      with torch.no_grad():
          predictions = policy_net(state)
      action = torch.argmax(predictions).item()
  return action

def update(loss_function, optimizer):
  if(len(memory) < 64):
    return
  target = []
  prediction = []
  replays = memory.sample(64)
  for replay in replays:
    prediction.append(torch.unsqueeze(policy_net(replay.state)[0, replay.action], 0))
    if replay.reward >= 0:
      target.append(torch.unsqueeze(replay.reward + discount * torch.max(target_net(replay.next_state)), 0))
    else:
      target.append(torch.unsqueeze(torch.tensor(replay.reward, device=device), 0))
  prediction = torch.cat(prediction)
  target = torch.cat(target)
  loss = loss_function(prediction, target)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
  return loss

update_iter = 0
update_step = 64
copy_weights_iter = 0
copy_weights_step = 512

lost = 0
scored = 0
iteration = 0
loss = 0

state = restart_game()

while True:
  reward = 0
  old_state = torch.clone(state)
  action = select_action(state)
  move_piece(state, action)
  if piece_landed(state):
    transfer_piece(state)
    if game_over():
      reward = -2
      lost += 1
      state = restart_game()
    else:
      scored_rows = score(state)
      scored += scored_rows
      reward = scored_rows
      reward += torch.sum(state[0, 1, game_rows-1, :])
      create_piece(state)
  
  memory.push(old_state, action, torch.clone(state), reward)

  if update_iter == update_step:
    loss = update(loss_function, optimizer)
    update_iter = 0
  update_iter += 1

  if copy_weights_iter == copy_weights_step:
    target_net.load_state_dict(policy_net.state_dict())
    target_net.eval()
    copy_weights_iter = 0
  copy_weights_iter += 1

  if iteration % 1000 == 0:
    print(f"--------------------\n")
    print(f"iteration: {iteration}")
    print(f"epsilon: {epsilon}")
    print(f"action: {action}")
    print(f"reward: {reward}")
    print(f"loss (last update): {loss}")
    print(f"lost: {lost}")
    print(f"scored: {scored}")
    print(f"\n{state}\n--------------------\n")
  iteration += 1